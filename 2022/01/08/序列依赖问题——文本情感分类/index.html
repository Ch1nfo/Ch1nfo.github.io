<!DOCTYPE html><html lang="zh-cn" theme-mode="dark"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>序列依赖问题——文本情感分类 | Chinfo's blog</title><link rel="icon" type="image/x-icon" href="/favicon.ico"><script>var config = {"root":"/","search":{"preload":false,"activeHolder":"键入以继续","blurHolder":"数据检索","noResult":"无 $0 相关数据"},"code":{"codeInfo":"$0 - $1 行","copy":"code.copy","copyFinish":"code.copyFinish","expand":"code.expand"}}</script><script src="//unpkg.com/mermaid@9.2.2/dist/mermaid.min.js"></script><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lightgallery.css"><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lg-zoom.css"><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lg-thumbnail.css"><link type="text/css" rel="stylesheet" href="/lib/fontawesome/css/all.min.css"><link rel="stylesheet" href="/css/arknights.css"><script>if (window.localStorage.getItem('theme-mode') === 'light')
 document.documentElement.setAttribute('theme-mode', 'light')
if (window.localStorage.getItem('theme-mode') === 'dark')
 document.documentElement.setAttribute('theme-mode', 'dark')</script><style>@font-face {
 font-family: Bender;
 src: local('Bender'), url("/font/Bender.ttf"), url("/font/Bender.otf");
}
@font-face {
 font-family: BenderLight;
 src: local('BenderLight'), url("/font/BenderLight.ttf");
}
@font-face {
 font-family: 'JetBrains Mono';
 src: local('JetBrains Mono'), url('/font/JetBrainsMono-Regular.woff2') format('woff2');
}</style><style>:root {
  --dark-background: url('https://ak.hypergryph.com/assets/index/images/ak/pc/bk.jpg');
  --light-background: url('/img/bk.jpg');
}</style><meta name="generator" content="Hexo 6.3.0"></head><body><div class="loading" style="opacity: 0;"><div class="loadingBar left"></div><div class="loadingBar right"></div></div><main><header class="closed"><div class="navBtn"><i class="navBtnIcon"><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span></i></div><nav><div class="navItem" id="search-header"><span class="navItemTitle"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="数据检索" spellcheck="false" maxlength="50" type="text" id="search-input"></span></div><div class="navItem" id="search-holder"></div><div class="search-popup"><div id="search-result"></div></div><ol class="navContent"><li class="navItem"><a class="navBlock" href="/"><span class="navItemTitle">Home</span></a></li><li class="navItem" matchdata="categories,tags"><a class="navBlock" href="/archives/"><span class="navItemTitle">Archives</span></a></li></ol></nav></header><article><div id="post-bg"><div id="post-title"><h1>序列依赖问题——文本情感分类</h1><div id="post-info"><span>文章发布时间: <div class="control"><time datetime="2022-01-08T13:54:02.000Z" id="date"> 2022-01-08</time></div></span><br><span>文章总字数: <div class="control">1.1k</div></span><br><span>预计阅读时间: <div class="control">4 分钟</div></span></div></div><hr><div id="post-content"><h1 id="序列依赖问题——文本情感分类"><a href="#序列依赖问题——文本情感分类" class="headerlink" title="序列依赖问题——文本情感分类"></a>序列依赖问题——文本情感分类</h1><p>之前的LeNet-5卷积神经网络模型体现了数据在空间上的关联性，但除此之外，数据也有可能在时间上有关联性，比如气温数据，股票数据等。当然，最典型的还是人类的语言。面对这样在时间上有关联的数据，神经网络该如何去识别和处理呢？</p>
<p>我们一般把词作为自然语言处理的基本单位，这样就可以利用“词典”用一个数字来代表一个词，这个数字就是索引值。这样，由词组成的一句话就会转化为一个由数字组成的向量，自然就可以送入神经网络进行识别了。</p>
<p>如果有些词在词典中由于种种原因数据接近，但是含义却相差甚远，这时候如果再进行归一化操作，计算机就很可能错误的识别到完全相反的意思。这样的数据就会给预测模型带来不必要的麻烦。所以我们可以使用one-hot的编码方式对词汇进行编码。通过one-hot编码后每个词都完全不一样。</p>
<p>但是这样进行严格区分的话，又丢失了词汇的关联性。我们可以提取出一个词汇的多个特征值形成一个词向量，即NLP中的词嵌入。</p>
<p>最后我们结合一下one-hot和词向量。让词向量组成的矩阵（字典）点乘按词语组成句子的one-hot编码形成的矩阵，这样就可以提取出句子中每一个词的词向量，这样就可以送入神经网络进行学习了。</p>
<p>明白了原理，我们就来实现一个简单的文本情感分类模型</p>
<p>首先在网上找到了一个别人爬的网购评论的csv文件，然后封装一个shoppingdata.py用来读取和处理这些数据。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">import os<br>import keras<br>import numpy as np<br>import keras.preprocessing.text as text<br>import re<br>import jieba<br>import random<br><br><br><br>def load_data():<br>	xs = []<br>	ys = []<br>	with open(os.path.dirname(os.path.abspath(__file__))+&#x27;/online_shopping_10_cats.csv&#x27;,&#x27;r&#x27;,encoding=&#x27;utf-8&#x27;) as f:<br>		line=f.readline()#escape first line&quot;label review&quot;<br>		while line:<br>			line=f.readline()<br>			if not line:<br>				break<br>			contents = line.split(&#x27;,&#x27;)<br><br>			# if contents[0]==&quot;书籍&quot;:<br>			# 	continue<br><br>			label = int(contents[1])<br>			review = contents[2]<br>			if len(review)&gt;20:<br>				continue<br><br>			xs.append(review)<br>			ys.append(label)<br><br>	xs = np.array(xs)<br>	ys = np.array(ys)<br><br>	#打乱数据集<br>	indies = [i for i in range(len(xs))] <br>	random.seed(666)<br>	random.shuffle(indies)<br>	xs = xs[indies]<br>	ys = ys[indies]<br><br>	m = len(xs)<br>	cutpoint = int(m*4/5)<br>	x_train = xs[:cutpoint]<br>	y_train = ys[:cutpoint]<br><br>	x_test = xs[cutpoint:]<br>	y_test = ys[cutpoint:]<br><br>	<br><br>	print(&#x27;总样本数量:%d&#x27; % (len(xs)))<br>	print(&#x27;训练集数量:%d&#x27; % (len(x_train)))<br>	print(&#x27;测试集数量:%d&#x27; % (len(x_test)))<br><br>	return x_train,y_train,x_test,y_test<br><br><br>def createWordIndex(x_train,x_test):<br>	x_all = np.concatenate((x_train,x_test),axis=0)<br>	#建立词索引<br>	tokenizer = text.Tokenizer()<br>	#create word index<br>	word_dic = &#123;&#125;<br>	voca = []<br>	for sentence in x_all:<br>	    # 去掉标点<br>	    sentence = re.sub(&quot;[\s+\.\!\/_,$%^*(+\&quot;\&#x27;]+|[+——！，。？、~@#￥%……&amp;*（）]+&quot;, &quot;&quot;, sentence)<br>	    # 结巴分词<br>	    cut = jieba.cut(sentence)<br>	    #cut_list = [ i for i in cut ]<br><br>	    for word in cut:<br>	    	if not (word in word_dic):<br>	    		word_dic[word]=0<br>	    	else:<br>	    		word_dic[word] +=1<br>	    	voca.append(word)<br>	word_dic = sorted(word_dic.items(), key = lambda kv:kv[1],reverse=True)<br><br>	voca = [v[0] for v in word_dic]<br>	<br>	tokenizer.fit_on_texts(voca)<br>	print(&quot;voca:&quot;+str(len(voca)))<br>	return len(voca),tokenizer.word_index<br><br>def word2Index(words,word_index):<br>	vecs = []<br>	for sentence in words:<br>	    # 去掉标点<br>	    sentence = re.sub(&quot;[\s+\.\!\/_,$%^*(+\&quot;\&#x27;]+|[+——！，。？、~@#￥%……&amp;*（）]+&quot;, &quot;&quot;, sentence)<br>	    # 结巴分词<br>	    cut = jieba.cut(sentence)<br>	    #cut_list = [ i for i in cut ]<br>	    index=[]<br><br>	    for word in cut:<br>	    	if word in word_index:<br>	    		index.append(float(word_index[word]))<br><br>	    # if len(index)&gt;25:<br>	    # 	index = index[0:25]<br>	    vecs.append(np.array(index))<br><br>	return np.array(vecs)<br></code></pre></td></tr></table></figure>

<p>首先得到数据的索引表示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">x_train_index = shopping_data.word2Index(x_train, word_index)<br>x_test_index = shopping_data.word2Index(x_test, word_index)<br></code></pre></td></tr></table></figure>

<p>因为每句话长短不同，我们需要利用keras.preprocessing中的sequence进行一个对齐操作</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">maxlen = 25<br>x_train_index = sequence.pad_sequences(x_train_index, maxlen=maxlen)<br>x_test_index = sequence.pad_sequences(x_test_index, maxlen=maxlen)<br></code></pre></td></tr></table></figure>

<p>首先创造一个Sequential模型，堆叠Embedding层，然后利用Flatten把数据平铺开，再送入一个256-256-256的神经网络中进行训练，最后利用sigmoid函数进行输出。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">model = Sequential()<br>model.add(Embedding(trainable=Ture, input_dim=vocalen, output_dim=300, input_length=maxlen))<br>model.add(Flatten())<br>model.add(Dense(256, activation=&#x27;relu&#x27;))<br>model.add(Dense(256, activation=&#x27;relu&#x27;))<br>model.add(Dense(256, activation=&#x27;relu&#x27;))<br><br><br>model.add(Dense(1, activation=&#x27;sigmoid&#x27;))<br></code></pre></td></tr></table></figure>

<p>然后使用交叉熵代价函数和adam优化器，训练200回合，批尺寸为512。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">model.compile(loss=&#x27;binary_crossentropy&#x27;, <br>              optimizer=&#x27;adam&#x27;,<br>              metrics=[&#x27;accuracy&#x27;])<br><br>model.fit(x_train_index, y_train,<br>		  batch_size=512,<br>		  epochs=200)<br></code></pre></td></tr></table></figure>

<p>然后开始训练。得到评估结果。</p>
<p class='item-img' data-src='https://cdn.jsdelivr.net/gh/Ch1nfo/picbed@main/img/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202022-01-08%20153810.png'><img src="https://cdn.jsdelivr.net/gh/Ch1nfo/picbed@main/img/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202022-01-08%20153810.png"></p>
<p>86%的准确度。</p>
<div id="paginator"></div></div><div id="post-footer"><div id="pages"><div class="footer-link" style="width: 50%;text-align:right;border-right:1px #fe2 solid"><a href="/2022/01/09/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89%E2%80%94%E2%80%94LSTM%E7%BD%91%E7%BB%9C/">← 下一篇 自然语言处理（NLP）——LSTM网络</a></div><div class="footer-link" style="width: 50%;right:1px;border-left:1px #fe2 solid"><a href="/2022/01/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">深度学习与卷积神经网络 上一篇 →</a></div></div></div></div><div class="bottom-btn"><div><a class="i-top" id="to-top" onClick="scrolls.scrolltop();" title="回到顶部" style="opacity: 0; display: none;">∧ </a><a class="i-index" id="to-index" href="#toc-div" title="文章目录">≡</a><a class="i-color" id="color-mode" onClick="colorMode.change()" title="切换主题"></a></div></div></article><aside><div id="about"><a href="/" id="logo"><img src="https://ak.hypergryph.com/assets/index/images/ak/pc/faction/1.png" alt="Logo"></a><h1 id="Dr"><a href="/">Chinfo's Blog</a></h1><div id="description"><p></p></div><div id="social-links"><a class="social" target="_blank" rel="noopener" href="https://github.com/Ch1nfo"><i class="fab fa-github" alt="GitHub"></i></a></div></div><div id="aside-block"><div id="toc-div"><h1>目录</h1><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E4%BE%9D%E8%B5%96%E9%97%AE%E9%A2%98%E2%80%94%E2%80%94%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB"><span class="toc-number">1.</span> <span class="toc-text">序列依赖问题——文本情感分类</span></a></li></ol></div></div><footer><nobr>构建自 <a target="_blank" rel="noopener" href="http://hexo.io">Hexo</a></nobr><wbr><nobr> 使用主题 <a target="_blank" rel="noopener" href="https://github.com/Yue-plus/hexo-theme-arknights">Arknights</a></nobr><wbr><nobr> 主题作者 <a target="_blank" rel="noopener" href="https://github.com/Yue-plus">Yue_plus</a></nobr></footer></aside></main><canvas id="canvas-dust"></canvas><script src="/js/search.js"></script><script src="/js/arknights.js"></script><script src="//unpkg.com/lightgallery@2.7.1/lightgallery.min.js"></script><script src="//unpkg.com/lightgallery@2.7.1/plugins/zoom/lg-zoom.min.js"></script><script src="//unpkg.com/lightgallery@2.7.1/plugins/thumbnail/lg-thumbnail.min.js"></script><script src="/js/pjax.js"></script><script class="pjax-js">reset= () => {document.querySelector('.lg-container')?.remove()
lightGallery(document.getElementById('post-bg'), {
  plugins: [lgZoom,lgThumbnail],
  selector: '.item-img'})}</script><script>window.addEventListener("load",() => {pjax = new Pjax({
 cacheBust: false,
 selectors: ['title','article','#aside-block','.pjax-js'],
 switches: {'article': Pjax.switches.sideBySide},
 switchesOptions: {
   'article': {
     classNames: {
       remove: "pjax-out",
       add: "pjax-in"
     }
   }
 }
});
document.addEventListener("pjax:complete", reset);reset()})</script></body></html>