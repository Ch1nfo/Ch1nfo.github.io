<!DOCTYPE html><html lang="zh-cn" theme-mode="dark"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>隐藏层与深度学习 | Chinfo's blog</title><link rel="icon" type="image/x-icon" href="/favicon.ico"><script>var config = {"root":"/","search":{"preload":false,"activeHolder":"键入以继续","blurHolder":"数据检索","noResult":"无 $0 相关数据"},"code":{"codeInfo":"$0 - $1 行","copy":"code.copy","copyFinish":"code.copyFinish","expand":"code.expand"}}</script><script src="//unpkg.com/mermaid@9.2.2/dist/mermaid.min.js"></script><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lightgallery.css"><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lg-zoom.css"><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lg-thumbnail.css"><link type="text/css" rel="stylesheet" href="/lib/fontawesome/css/all.min.css"><link rel="stylesheet" href="/css/arknights.css"><script>if (window.localStorage.getItem('theme-mode') === 'light')
 document.documentElement.setAttribute('theme-mode', 'light')
if (window.localStorage.getItem('theme-mode') === 'dark')
 document.documentElement.setAttribute('theme-mode', 'dark')</script><style>@font-face {
 font-family: Bender;
 src: local('Bender'), url("/font/Bender.ttf"), url("/font/Bender.otf");
}
@font-face {
 font-family: BenderLight;
 src: local('BenderLight'), url("/font/BenderLight.ttf");
}
@font-face {
 font-family: 'JetBrains Mono';
 src: local('JetBrains Mono'), url('/font/JetBrainsMono-Regular.woff2') format('woff2');
}</style><style>:root {
  --dark-background: url('https://ak.hypergryph.com/assets/index/images/ak/pc/bk.jpg');
  --light-background: url('/img/bk.jpg');
}</style><meta name="generator" content="Hexo 6.3.0"></head><body><div class="loading" style="opacity: 0;"><div class="loadingBar left"></div><div class="loadingBar right"></div></div><main><header class="closed"><div class="navBtn"><i class="navBtnIcon"><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span></i></div><nav><div class="navItem" id="search-header"><span class="navItemTitle"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="数据检索" spellcheck="false" maxlength="50" type="text" id="search-input"></span></div><div class="navItem" id="search-holder"></div><div class="search-popup"><div id="search-result"></div></div><ol class="navContent"><li class="navItem"><a class="navBlock" href="/"><span class="navItemTitle">Home</span></a></li><li class="navItem" matchdata="categories,tags"><a class="navBlock" href="/archives/"><span class="navItemTitle">Archives</span></a></li></ol></nav></header><article><div id="post-bg"><div id="post-title"><h1>隐藏层与深度学习</h1><div id="post-info"><span>文章发布时间: <div class="control"><time datetime="2022-01-03T13:54:02.000Z" id="date"> 2022-01-03</time></div></span><br><span>文章总字数: <div class="control">850</div></span><br><span>预计阅读时间: <div class="control">3 分钟</div></span></div></div><hr><div id="post-content"><h1 id="隐藏层与深度学习"><a href="#隐藏层与深度学习" class="headerlink" title="隐藏层与深度学习"></a>隐藏层与深度学习</h1><p>在现实中，判断一个物体，或者说对一个物体下定义，往往不能只从一个方面或两个方面观察。这时候，单个的Rosenblatt感知器已经无法胜任这样复杂的判断，是时候再增加一个神经元了。新增一个神经元，便增加了一个抽象的维度，每个维度通过不断地调整权重并进行激活，从而产生对输入的不同理解，最后再将这些抽象维度中的输出合并并降维得到最后的输出。而这些新添加的神经元，就被称之为隐藏层。</p>
<p>为了直观的看到效果，我还是使用了matplotlib进行绘图。这一次的w和b的值，我选择使用rand()函数生成随机数。并且为了之后的方便，将sigmoid函数（标准Logistic函数）和前向传播封装为两个函数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">import dataset<br>import matplotlib.pyplot as plt<br>import numpy as np<br><br>m = 100<br>xs, ys = dataset.get_beans(m)<br><br><br><br>plt.title(&quot;STF&quot;, fontsize=12)<br>plt.xlabel(&quot;B&quot;)<br>plt.ylabel(&quot;T&quot;)<br><br>plt.scatter(xs, ys)<br><br>def sigmoid(x):<br>	return 1/(1+np.exp(-x))<br><br><br><br>#第一层<br>#第一个神经元<br>w11_1 = np.random.rand()<br>b1_1 = np.random.rand()<br>#第二个神经元<br>w12_1 = np.random.rand()<br>b2_1 = np.random.rand()<br><br>#第二层<br>w11_2 = np.random.rand()<br>w21_2 = np.random.rand()<br>b1_2 = np.random.rand()<br><br>#前向传播<br>def forward(xs):<br>	#第一层第一个<br>	z1_1 = w11_1*xs + b1_1<br>	a1_1 = sigmoid(z1_1)<br>	#第一层第二个<br>	z2_1 = w12_1*xs + b2_1<br>	a2_1 = sigmoid(z2_1)<br>	#第二层（输出层）<br>	z1_2 = w11_2*a1_1 + w21_2*a2_1 + b1_2<br>	a1_2 = sigmoid(z1_2)<br>	return a1_2,z1_2,a2_1,z2_1,a1_1,z1_1<br></code></pre></td></tr></table></figure>

<p>这样准备工作就完成了，接下来就是繁琐的隐藏层求导进行反向传播的过程。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">for _ in range(5000):<br>	for i in range(100):<br>		x = xs[i]<br>		y = ys[i]<br>		#先来一次前向传播<br>		a1_2,z1_2,a2_1,z2_1,a1_1,z1_1 = forward(x)<br>		#反向传播<br>		#代价函数e<br>		e = (y-a1_2)**2<br><br>		deda1_2 = -2*(y - a1_2)<br>		da1_2dz1_2 = a1_2*(1 - a1_2)<br><br>		dz1_2dw11_2 = a1_1<br>		dz1_2dw21_2 = a2_1<br><br>		dedw11_2 = deda1_2*da1_2dz1_2*dz1_2dw11_2<br>		dedw21_2 = deda1_2*da1_2dz1_2*dz1_2dw21_2<br><br>		dz1_2db1_2 = 1<br>		dedb1_2 = deda1_2*da1_2dz1_2*dz1_2db1_2<br><br>		#隐藏层的第一个神经元<br>		dz1_2da1_1 = w11_2<br>		da1_1dz1_1 = a1_1*(1-a1_1)<br>		dz1_1dw11_1 = x<br><br>		dedw11_1 = deda1_2*da1_2dz1_2*dz1_2da1_1*da1_1dz1_1*dz1_1dw11_1<br>		dz1_1db1_1 = 1<br>		dedb1_1 = deda1_2*da1_2dz1_2*dz1_2da1_1*da1_1dz1_1*dz1_1db1_1<br><br>		#隐藏层的第二个神经元<br>		dz1_2da2_1 = w21_2<br>		da2_1dz2_1 = a2_1*(1-a2_1)<br>		dz2_1dw12_1 = x<br><br>		dedw12_1 = deda1_2*da1_2dz1_2*dz1_2da2_1*da2_1dz2_1*dz2_1dw12_1<br>		dz2_1db2_1 = 1<br>		dedb2_1 = deda1_2*da1_2dz1_2*dz1_2da2_1*da2_1dz2_1*dz2_1db2_1<br><br>		#梯度下降<br>		alpha = 0.03<br>		w11_2 = w11_2 - alpha*dedw11_2<br>		w21_2 = w21_2 - alpha*dedw21_2<br>		b1_2 = b1_2 - alpha*dedb1_2<br><br>		w12_1 = w12_1 - alpha*dedw12_1<br>		b2_1 = b2_1 - alpha*dedb2_1<br><br>		w11_1 = w11_1 - alpha*dedw11_1<br>		b1_1 = b1_1 - alpha*dedb1_1<br></code></pre></td></tr></table></figure>

<p>经过漫长的求导与修改错误，最后再使用plt.clf()函数和plt.pause()函数相结合绘制动态图像，就得到了最终的结果。</p>
<p class='item-img' data-src='https://cdn.jsdelivr.net/gh/Ch1nfo/picbed@main/img/hidden.gif'><img src="https://cdn.jsdelivr.net/gh/Ch1nfo/picbed@main/img/hidden.gif"></p>
<p>当隐藏层变得更多与更深，神经网络也会变得更加强大与智能。当我们设计出一个复杂程度恰当的神经网络，经过不断的训练过后，网络中的各个参数被调节成不同的值，而他们组成的整体，就可以近似出一个相当复杂的函数。</p>
<p>我们一般把隐藏层超过三层的网络称之为深度神经网络。隐藏层的神经元对样本的理解和提取的参数都太过微妙，我们能做的也只有设计一个网络，送入数据进行训练，如果效果好，那它就”起作用了“，效果不好就”没起作用“，只能再次调参，再来一次。这也是为什么深度学习被很多人戏称”炼丹“的原因。</p>
<div id="paginator"></div></div><div id="post-footer"><div id="pages"><div class="footer-link" style="width: 50%;text-align:right;border-right:1px #fe2 solid"><a href="/2022/01/03/%E7%AE%80%E5%8D%95%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/">← 下一篇 简单的梯度下降算法实现</a></div><div class="footer-link" style="width: 50%;right:1px;border-left:1px #fe2 solid"><a href="/2022/01/02/%E7%AE%80%E5%8D%95%E7%9A%84Rosenblatt%E6%84%9F%E7%9F%A5%E5%99%A8%E4%B8%8E%E4%BA%8C%E7%BB%B4%E6%96%B9%E5%B7%AE%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0/">简单的Rosenblatt感知器与二维方差代价函数 上一篇 →</a></div></div></div></div><div class="bottom-btn"><div><a class="i-top" id="to-top" onClick="scrolls.scrolltop();" title="回到顶部" style="opacity: 0; display: none;">∧ </a><a class="i-index" id="to-index" href="#toc-div" title="文章目录">≡</a><a class="i-color" id="color-mode" onClick="colorMode.change()" title="切换主题"></a></div></div></article><aside><div id="about"><a href="/" id="logo"><img src="https://ak.hypergryph.com/assets/index/images/ak/pc/faction/1.png" alt="Logo"></a><h1 id="Dr"><a href="/">Chinfo's Blog</a></h1><div id="description"><p></p></div><div id="social-links"><a class="social" target="_blank" rel="noopener" href="https://github.com/Ch1nfo"><i class="fab fa-github" alt="GitHub"></i></a></div></div><div id="aside-block"><div id="toc-div"><h1>目录</h1><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%9A%90%E8%97%8F%E5%B1%82%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.</span> <span class="toc-text">隐藏层与深度学习</span></a></li></ol></div></div><footer><nobr>构建自 <a target="_blank" rel="noopener" href="http://hexo.io">Hexo</a></nobr><wbr><nobr> 使用主题 <a target="_blank" rel="noopener" href="https://github.com/Yue-plus/hexo-theme-arknights">Arknights</a></nobr><wbr><nobr> 主题作者 <a target="_blank" rel="noopener" href="https://github.com/Yue-plus">Yue_plus</a></nobr></footer></aside></main><canvas id="canvas-dust"></canvas><script src="/js/search.js"></script><script src="/js/arknights.js"></script><script src="//unpkg.com/lightgallery@2.7.1/lightgallery.min.js"></script><script src="//unpkg.com/lightgallery@2.7.1/plugins/zoom/lg-zoom.min.js"></script><script src="//unpkg.com/lightgallery@2.7.1/plugins/thumbnail/lg-thumbnail.min.js"></script><script src="/js/pjax.js"></script><script class="pjax-js">reset= () => {document.querySelector('.lg-container')?.remove()
lightGallery(document.getElementById('post-bg'), {
  plugins: [lgZoom,lgThumbnail],
  selector: '.item-img'})}</script><script>window.addEventListener("load",() => {pjax = new Pjax({
 cacheBust: false,
 selectors: ['title','article','#aside-block','.pjax-js'],
 switches: {'article': Pjax.switches.sideBySide},
 switchesOptions: {
   'article': {
     classNames: {
       remove: "pjax-out",
       add: "pjax-in"
     }
   }
 }
});
document.addEventListener("pjax:complete", reset);reset()})</script></body></html>